
#' @importFrom utils bibentry

bibentries = c(
  greenwell_simple_2018 = bibentry("techreport",
    title = "A Simple and Effective Model-Based Variable Importance Measure",
    url = "http://arxiv.org/abs/1805.04755",
    institution = "arXiv preprint",
    author = "Greenwell, Brandon M. and Boehmke, Bradley C. and McCarthy, Andrew J.",
    year = "2018",
    note = "arXiv:1805.04755"
  ),
  molnar_relating_2023 = bibentry("inproceedings",
    address = "Cham",
    series = "Communications in Computer and Information Science",
    title = "Relating the Partial Dependence Plot and Permutation Feature Importance to the Data Generating Process",
    isbn = "978-3-031-44064-9",
    # doi = "10.1007/978-3-031-44064-9_24",
    booktitle = "Explainable Artificial Intelligence",
    publisher = "Springer Nature Switzerland",
    author = "Molnar, Christoph and Freiesleben, Timo and Koenig, Gunnar and Herbinger, Julia and Reisinger, Tim and Casalicchio, Giuseppe and Wright, Marvin N. and Bischl, Bernd",
    editor = "Longo, Luca",
    year = "2023",
    keywords = "XAI, Interpretable Machine Learning, Partial Dependence Plot, Permutation Feature Importance, Statistical Inference, Uncertainty Quantification",
    pages = "456--479",
  ),
  fisher_all_2019 = bibentry("inproceedings",
    title = "All Models are Wrong, but Many are Useful: Learning a Variable’s Importance by Studying an Entire Class of Prediction Models Simultaneously",
    author = "Lundberg, Scott M. and Lee, Su-In",
    title = "A unified approach to interpreting model predictions",
    year = "2017",
    isbn = "9781510860964",
    publisher = "Curran Associates Inc.",
    address = "Red Hook, NY, USA",
    abstract = "Understanding why a model makes a certain prediction can be as crucial as the prediction's accuracy in many applications. However, the highest accuracy for large modern datasets is often achieved by complex models that even experts struggle to interpret, such as ensemble or deep learning models, creating a tension between accuracy and interpretability. In response, various methods have recently been proposed to help users interpret the predictions of complex models, but it is often unclear how these methods are related and when one method is preferable over another. To address this problem, we present a unified framework for interpreting predictions, SHAP (SHapley Additive exPlanations). SHAP assigns each feature an importance value for a particular prediction. Its novel components include: (1) the identification of a new class of additive feature importance measures, and (2) theoretical results showing there is a unique solution in this class with a set of desirable properties. The new class unifies six existing methods, notable because several recent methods in the class lack the proposed desirable properties. Based on insights from this unification, we present new methods that show improved computational performance and/or better consistency with human intuition than previous approaches.",
    booktitle = "Proceedings of the 31st International Conference on Neural Information Processing Systems",
    pages = "4768–4777",
    numpages = "10",
    location = "Long Beach, California, USA",
    series = "NIPS'17"
  ), breiman_leo_random_2001 = bibentry("article",
    title = "Random Forest",
    volume = "45",
    # doi = "10.1023/a:1010933404324",
    number = "1",
    journal = "Machine Learning",
    author = "Breiman, Leo",
    year = "2001",
    pages = "5--32",
  )
)
